{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data size', 17005207)\n"
     ]
    }
   ],
   "source": [
    "with open('./text8', 'r') as f:\n",
    "    words = tf.compat.as_str(f.read()).split()\n",
    "print('Data size', len(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common words (+UNK)', [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)])\n",
      "('Sample data', [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156], ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against'])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 45848,\n",
       " 'homomorphism': 9648,\n",
       " 'nordisk': 39343,\n",
       " 'nunnery': 36075,\n",
       " 'chthonic': 33554,\n",
       " 'sowell': 40562,\n",
       " 'sonja': 38175,\n",
       " 'showa': 32906,\n",
       " 'woods': 6263,\n",
       " 'hsv': 44222,\n",
       " 'spiders': 14623,\n",
       " 'hanging': 8021,\n",
       " 'woody': 11150,\n",
       " 'comically': 38935,\n",
       " 'localized': 16716,\n",
       " 'schlegel': 39763,\n",
       " 'sevens': 47931,\n",
       " 'canes': 30965,\n",
       " 'sprague': 19496,\n",
       " 'chatter': 45028,\n",
       " 'orthographies': 37085,\n",
       " 'nanking': 40997,\n",
       " 'originality': 19567,\n",
       " 'mutinies': 47932,\n",
       " 'alphabetic': 13925,\n",
       " 'hermann': 6482,\n",
       " 'cytochrome': 26060,\n",
       " 'brocklin': 43516,\n",
       " 'stipulate': 42056,\n",
       " 'eugenics': 7421,\n",
       " 'cofinality': 36724,\n",
       " 'danmarks': 46453,\n",
       " 'capoeira': 8971,\n",
       " 'appropriation': 21010,\n",
       " 'holyrood': 49046,\n",
       " 'taj': 29360,\n",
       " 'strictest': 30374,\n",
       " 'bringing': 3627,\n",
       " 'wooded': 18967,\n",
       " 'liaisons': 38573,\n",
       " 'vibrational': 25367,\n",
       " 'umts': 33555,\n",
       " 'wooden': 4779,\n",
       " 'wednesday': 13043,\n",
       " 'circuitry': 14992,\n",
       " 'elgar': 16456,\n",
       " 'stereotypical': 15093,\n",
       " 'immunities': 38936,\n",
       " 'pantheistic': 30327,\n",
       " 'thrace': 12647,\n",
       " 'kublai': 30769,\n",
       " 'insular': 13743,\n",
       " 'grainy': 48065,\n",
       " 'francesco': 8358,\n",
       " 'feasibility': 21379,\n",
       " 'miniatures': 33046,\n",
       " 'deadheads': 44612,\n",
       " 'selassie': 16717,\n",
       " 'gorman': 38176,\n",
       " 'sustaining': 18689,\n",
       " 'consenting': 36077,\n",
       " 'kodak': 27685,\n",
       " 'prosody': 30543,\n",
       " 'inanimate': 16353,\n",
       " 'dormancy': 46481,\n",
       " 'errors': 4065,\n",
       " 'semicircular': 35237,\n",
       " 'tiered': 33047,\n",
       " 'centimeter': 35090,\n",
       " 'cooking': 5083,\n",
       " 'paphos': 46482,\n",
       " 'usenet': 7637,\n",
       " 'vassals': 16425,\n",
       " 'designing': 8827,\n",
       " 'numeral': 8644,\n",
       " 'succumb': 45241,\n",
       " 'shocks': 18089,\n",
       " 'evolutionism': 21633,\n",
       " 'widget': 25768,\n",
       " 'crouch': 37790,\n",
       " 'moksha': 24751,\n",
       " 'ibizan': 48812,\n",
       " 'brainwashed': 45849,\n",
       " 'affiliates': 22515,\n",
       " 'ching': 9021,\n",
       " 'mutinied': 45240,\n",
       " 'china': 486,\n",
       " 'galeon': 39345,\n",
       " 'affiliated': 7121,\n",
       " 'confronts': 31315,\n",
       " 'finkel': 43517,\n",
       " 'natured': 31211,\n",
       " 'quart': 43747,\n",
       " 'kids': 7809,\n",
       " 'ansgar': 38177,\n",
       " 'uplifting': 40563,\n",
       " 'mechagodzilla': 32394,\n",
       " 'climbed': 17582,\n",
       " 'controversy': 1549,\n",
       " 'kidd': 14023,\n",
       " 'natures': 18345,\n",
       " 'neurologist': 32815,\n",
       " 'spotty': 41455,\n",
       " 'climber': 21075,\n",
       " 'gottlob': 29560,\n",
       " 'millimetres': 29561,\n",
       " 'golden': 1433,\n",
       " 'topography': 12061,\n",
       " 'projection': 7024,\n",
       " 'wikimedia': 29185,\n",
       " 'lengthen': 30329,\n",
       " 'spacewalks': 43518,\n",
       " 'stern': 11253,\n",
       " 'battista': 19629,\n",
       " 'agassi': 10639,\n",
       " 'dna': 1403,\n",
       " 'catchy': 30330,\n",
       " 'insecurity': 21634,\n",
       " 'abbreviations': 9731,\n",
       " 'grahame': 43529,\n",
       " 'cannibal': 24086,\n",
       " 'definiteness': 38178,\n",
       " 'music': 166,\n",
       " 'therefore': 589,\n",
       " 'dns': 7041,\n",
       " 'mystic': 10296,\n",
       " 'dusting': 43735,\n",
       " 'sermons': 10237,\n",
       " 'ungulates': 25920,\n",
       " 'duals': 45850,\n",
       " 'populations': 2970,\n",
       " 'watercolor': 38568,\n",
       " 'crossbar': 31418,\n",
       " 'takla': 35809,\n",
       " 'yahoo': 7089,\n",
       " 'meteorologist': 38937,\n",
       " 'expeditionary': 19497,\n",
       " 'primeval': 29186,\n",
       " 'tlp': 37729,\n",
       " 'conch': 44040,\n",
       " 'circumstances': 2780,\n",
       " 'hj': 36140,\n",
       " 'intake': 8102,\n",
       " 'morally': 12781,\n",
       " 'locked': 9980,\n",
       " 'arndt': 39346,\n",
       " 'heathland': 45851,\n",
       " 'locker': 38575,\n",
       " 'gershom': 42434,\n",
       " 'tuatha': 27044,\n",
       " 'confining': 35568,\n",
       " 'bocks': 47933,\n",
       " 'matilda': 12059,\n",
       " 'subgenus': 30966,\n",
       " 'wang': 10546,\n",
       " 'wand': 28975,\n",
       " 'wane': 34356,\n",
       " 'unjust': 17790,\n",
       " 'progenitors': 38195,\n",
       " 'castell': 43133,\n",
       " 'titanium': 18498,\n",
       " 'want': 2154,\n",
       " 'carew': 43519,\n",
       " 'pinto': 28630,\n",
       " 'cookery': 41970,\n",
       " 'absolute': 2277,\n",
       " 'ferranti': 37489,\n",
       " 'wracked': 49401,\n",
       " 'tchaikovsky': 32816,\n",
       " 'beyer': 47137,\n",
       " 'travel': 1568,\n",
       " 'leutze': 42970,\n",
       " 'copious': 23112,\n",
       " 'playback': 12326,\n",
       " 'actionscript': 35528,\n",
       " 'canaries': 25769,\n",
       " 'cadence': 26061,\n",
       " 'addington': 45561,\n",
       " 'shaitan': 31893,\n",
       " 'assimilated': 13675,\n",
       " 'apocrypha': 10874,\n",
       " 'drenthe': 42971,\n",
       " 'festus': 41456,\n",
       " 'dinosaurs': 6107,\n",
       " 'wrong': 2822,\n",
       " 'sentencing': 21794,\n",
       " 'perutz': 42972,\n",
       " 'bratislava': 14429,\n",
       " 'arranger': 24508,\n",
       " 'vukovar': 42658,\n",
       " 'recombination': 16543,\n",
       " 'baldr': 19949,\n",
       " 'tulip': 27201,\n",
       " 'menlo': 32357,\n",
       " 'torts': 46601,\n",
       " 'carelessness': 47866,\n",
       " 'nonsensical': 20712,\n",
       " 'macmullen': 42444,\n",
       " 'welcomed': 10785,\n",
       " 'stoicism': 40564,\n",
       " 'partnered': 29188,\n",
       " 'concurrency': 28631,\n",
       " 'rewarded': 13988,\n",
       " 'sentry': 30862,\n",
       " 'activating': 25637,\n",
       " 'playhouse': 26449,\n",
       " 'fir': 19751,\n",
       " 'fis': 29373,\n",
       " 'ligand': 14731,\n",
       " 'fit': 2641,\n",
       " 'fractal': 8995,\n",
       " 'fix': 8041,\n",
       " 'meantone': 26143,\n",
       " 'uriah': 32596,\n",
       " 'secede': 20369,\n",
       " 'fib': 47138,\n",
       " 'fic': 37431,\n",
       " 'fia': 22154,\n",
       " 'fig': 14260,\n",
       " 'fie': 37139,\n",
       " 'wales': 1946,\n",
       " 'menachem': 19686,\n",
       " 'fin': 13903,\n",
       " 'fim': 37432,\n",
       " 'songwriter': 4446,\n",
       " 'vouchers': 23737,\n",
       " 'hypoxic': 42447,\n",
       " 'hypoxia': 30544,\n",
       " 'effects': 769,\n",
       " 'multidimensional': 21212,\n",
       " 'sixteen': 6863,\n",
       " 'undeveloped': 23638,\n",
       " 'saddened': 40178,\n",
       " 'ulmanis': 46806,\n",
       " 'honeybee': 32358,\n",
       " 'barton': 10524,\n",
       " 'telekom': 45852,\n",
       " 'bartos': 44627,\n",
       " 'rabba': 35354,\n",
       " 'arrow': 5900,\n",
       " 'ingrid': 32359,\n",
       " 'burial': 5500,\n",
       " 'diatoms': 44106,\n",
       " 'telescope': 6169,\n",
       " 'zuid': 36079,\n",
       " 'allah': 6099,\n",
       " 'allan': 6941,\n",
       " 'phoenicians': 19374,\n",
       " 'parasites': 14921,\n",
       " 'strips': 5656,\n",
       " 'internalism': 42973,\n",
       " 'veracruz': 24294,\n",
       " 'tricked': 26221,\n",
       " 'stabilised': 32597,\n",
       " 'adapa': 34357,\n",
       " 'golem': 12453,\n",
       " 'mannerheim': 25638,\n",
       " 'unital': 27488,\n",
       " 'syd': 33476,\n",
       " 'mason': 7035,\n",
       " 'combinatorial': 17838,\n",
       " 'encourage': 5501,\n",
       " 'adapt': 9964,\n",
       " 'unitas': 18810,\n",
       " 'guitarists': 10271,\n",
       " 'outburst': 38576,\n",
       " 'abbott': 10402,\n",
       " 'stamping': 38577,\n",
       " 'abbots': 16109,\n",
       " 'strata': 14398,\n",
       " 'expressiveness': 46905,\n",
       " 'pumpkins': 29374,\n",
       " 'corrects': 37791,\n",
       " 'estimate': 4447,\n",
       " 'gameplay': 8618,\n",
       " 'universally': 5362,\n",
       " 'chlorine': 9629,\n",
       " 'husbands': 17627,\n",
       " 'competes': 19437,\n",
       " 'usurpers': 45041,\n",
       " 'nephites': 31212,\n",
       " 'pfaff': 47139,\n",
       " 'flicker': 46869,\n",
       " 'ministries': 11307,\n",
       " 'disturbed': 12699,\n",
       " 'competed': 11132,\n",
       " 'portmanteau': 24624,\n",
       " 'loudness': 32137,\n",
       " 'lightened': 42616,\n",
       " 'exoplanets': 44500,\n",
       " 'federko': 36390,\n",
       " 'reintroduced': 20715,\n",
       " 'humanitarians': 35811,\n",
       " 'pamphilus': 39765,\n",
       " 'mesmer': 20783,\n",
       " 'kashima': 47935,\n",
       " 'megabytes': 22893,\n",
       " 'feldspar': 32360,\n",
       " 'discworld': 10733,\n",
       " 'trochaic': 43521,\n",
       " 'dupuis': 40999,\n",
       " 'antilles': 12670,\n",
       " 'douard': 25306,\n",
       " 'seizures': 9948,\n",
       " 'rett': 44880,\n",
       " 'olds': 20848,\n",
       " 'bakelite': 39766,\n",
       " 'renovated': 19498,\n",
       " 'service': 351,\n",
       " 'forrester': 26450,\n",
       " 'reuben': 17684,\n",
       " 'needed': 1262,\n",
       " 'master': 1418,\n",
       " 'dddddd': 41303,\n",
       " 'genesis': 3694,\n",
       " 'berlioz': 39767,\n",
       " 'rewards': 13005,\n",
       " 'voight': 30102,\n",
       " 'organically': 41537,\n",
       " 'formant': 33712,\n",
       " 'pham': 47190,\n",
       " 'doreen': 43050,\n",
       " 'mutilated': 24855,\n",
       " 'icbms': 23181,\n",
       " 'positively': 12373,\n",
       " 'ahmed': 7891,\n",
       " 'maglev': 41473,\n",
       " 'bannister': 38574,\n",
       " 'duckworth': 33801,\n",
       " 'awacs': 35239,\n",
       " 'anniversaries': 28280,\n",
       " 'regulator': 27202,\n",
       " 'idle': 9169,\n",
       " 'exclaimed': 25639,\n",
       " 'sheen': 21995,\n",
       " 'boudica': 18391,\n",
       " 'silky': 40455,\n",
       " 'feeling': 4316,\n",
       " 'citeaux': 49404,\n",
       " 'herbalists': 49405,\n",
       " 'codeine': 20422,\n",
       " 'pliocene': 29563,\n",
       " 'spectrum': 2722,\n",
       " 'pandemics': 38179,\n",
       " 'increment': 22343,\n",
       " 'arousal': 27203,\n",
       " 'thaw': 38107,\n",
       " 'urinate': 49406,\n",
       " 'gyeonggi': 40975,\n",
       " 'nmi': 45883,\n",
       " 'dozen': 6751,\n",
       " 'foundational': 14186,\n",
       " 'affairs': 1704,\n",
       " 'scraped': 33556,\n",
       " 'wholesome': 32361,\n",
       " 'courier': 18881,\n",
       " 'hymen': 43523,\n",
       " 'sein': 34117,\n",
       " 'beltway': 31846,\n",
       " 'vga': 41457,\n",
       " 'primed': 45469,\n",
       " 'racers': 20645,\n",
       " 'toothed': 24859,\n",
       " 'kremlin': 18743,\n",
       " 'shipments': 21149,\n",
       " 'inoue': 36667,\n",
       " 'committing': 13439,\n",
       " 'sugarcane': 17791,\n",
       " 'limitless': 32598,\n",
       " 'diminishing': 18392,\n",
       " 'cinematic': 15274,\n",
       " 'retroviruses': 35812,\n",
       " 'metrics': 21720,\n",
       " 'simplify': 14853,\n",
       " 'mouth': 2827,\n",
       " 'dissociatives': 37794,\n",
       " 'reverence': 15946,\n",
       " 'conceded': 19828,\n",
       " 'resonated': 39768,\n",
       " 'expanse': 37205,\n",
       " 'proliferate': 36459,\n",
       " 'signalled': 21762,\n",
       " 'gaucho': 40406,\n",
       " 'bradford': 15164,\n",
       " 'treize': 40070,\n",
       " 'singer': 768,\n",
       " 'khattab': 42454,\n",
       " 'purges': 17730,\n",
       " 'vertices': 10365,\n",
       " 'ragnarok': 32817,\n",
       " 'multiracial': 31654,\n",
       " 'tech': 7294,\n",
       " 'anabaptists': 13179,\n",
       " 'cyrene': 27047,\n",
       " 'rakis': 34683,\n",
       " 'scream': 15685,\n",
       " 'saying': 1977,\n",
       " 'blatantly': 31405,\n",
       " 'dickey': 30107,\n",
       " 'teresa': 15426,\n",
       " 'jocular': 49729,\n",
       " 'rediscovery': 18070,\n",
       " 'hillbillies': 24267,\n",
       " 'fennel': 37089,\n",
       " 'ulcer': 41972,\n",
       " 'tempted': 27958,\n",
       " 'cheaply': 23114,\n",
       " 'eliminated': 5280,\n",
       " 'orleans': 5033,\n",
       " 'faure': 41483,\n",
       " 'clicked': 45853,\n",
       " 'nontrinitarian': 48679,\n",
       " 'kinase': 23331,\n",
       " 'blish': 24516,\n",
       " 'rico': 8195,\n",
       " 'statehouse': 43524,\n",
       " 'bliss': 12343,\n",
       " 'rick': 7209,\n",
       " 'rich': 1611,\n",
       " 'macrinus': 22335,\n",
       " 'rice': 2973,\n",
       " 'rica': 5363,\n",
       " 'caribs': 26451,\n",
       " 'plate': 3257,\n",
       " 'plata': 22337,\n",
       " 'plato': 5224,\n",
       " 'umbilical': 25770,\n",
       " 'firenze': 29385,\n",
       " 'platt': 25368,\n",
       " 'photoelectric': 25098,\n",
       " 'altogether': 6118,\n",
       " 'bhfiann': 33315,\n",
       " 'platz': 36726,\n",
       " 'superfamily': 19631,\n",
       " 'jaguar': 8619,\n",
       " 'rectal': 41472,\n",
       " 'nicely': 36727,\n",
       " 'ennedi': 40565,\n",
       " 'boarder': 45244,\n",
       " 'patch': 7928,\n",
       " 'eyelids': 47937,\n",
       " 'ldp': 19874,\n",
       " 'boarded': 22630,\n",
       " 'aikidoka': 38180,\n",
       " 'conant': 47944,\n",
       " 'programmatic': 40566,\n",
       " 'scolded': 47644,\n",
       " 'clarified': 16959,\n",
       " 'sensitivity': 8492,\n",
       " 'irritable': 26928,\n",
       " 'tyndale': 25099,\n",
       " 'eschewing': 47938,\n",
       " 'erika': 33557,\n",
       " 'tyndall': 25100,\n",
       " 'clarifies': 40179,\n",
       " 'shunting': 43415,\n",
       " 'playfulness': 46485,\n",
       " 'lots': 7853,\n",
       " 'irr': 37433,\n",
       " 'irs': 26753,\n",
       " 'irt': 44614,\n",
       " 'iru': 37434,\n",
       " 'irv': 16110,\n",
       " 'lott': 28976,\n",
       " 'xvi': 10117,\n",
       " 'loti': 39769,\n",
       " 'irl': 42435,\n",
       " 'irn': 18744,\n",
       " 'conductive': 19223,\n",
       " 'ira': 4780,\n",
       " 'adnan': 28923,\n",
       " 'irc': 4925,\n",
       " 'proteases': 39468,\n",
       " 'ire': 20565,\n",
       " 'volap': 49983,\n",
       " 'discipline': 3578,\n",
       " 'redistricting': 35529,\n",
       " 'natura': 28633,\n",
       " 'extend': 3871,\n",
       " 'nature': 544,\n",
       " 'pharisee': 38181,\n",
       " 'extent': 1869,\n",
       " 'tendons': 40567,\n",
       " 'inflorescence': 43525,\n",
       " 'tyranny': 14293,\n",
       " 'airflow': 16544,\n",
       " 'veer': 35813,\n",
       " 'seleucids': 39770,\n",
       " 'himalayas': 21878,\n",
       " 'heating': 6236,\n",
       " 'incense': 28121,\n",
       " 'fruity': 36489,\n",
       " 'wenham': 45154,\n",
       " 'himalayan': 22997,\n",
       " 'southeastern': 7317,\n",
       " 'moravia': 11401,\n",
       " 'eradicate': 21721,\n",
       " 'libyan': 8421,\n",
       " 'assur': 39771,\n",
       " 'gopher': 11908,\n",
       " 'gypsies': 21076,\n",
       " 'rooks': 49613,\n",
       " 'knossos': 22517,\n",
       " 'basque': 3200,\n",
       " 'intrauterine': 35336,\n",
       " 'blonde': 11888,\n",
       " 'organisational': 27204,\n",
       " 'godhead': 19687,\n",
       " 'fra': 14854,\n",
       " 'frg': 35530,\n",
       " 'penderecki': 32908,\n",
       " 'icebergs': 49554,\n",
       " 'union': 266,\n",
       " 'guybrush': 30358,\n",
       " 'fro': 41459,\n",
       " 'cue': 7223,\n",
       " 'enumerative': 45245,\n",
       " 'frs': 23967,\n",
       " 'much': 149,\n",
       " 'wyman': 37795,\n",
       " 'progenitor': 24752,\n",
       " 'unleavened': 34358,\n",
       " 'fry': 14316,\n",
       " 'tallest': 7580,\n",
       " 'meribbaal': 46432,\n",
       " 'obese': 27959,\n",
       " 'retrospect': 22429,\n",
       " 'spit': 20171,\n",
       " 'cotangent': 30332,\n",
       " 'conifers': 33558,\n",
       " 'freehold': 33327,\n",
       " 'davy': 10956,\n",
       " 'dave': 3736,\n",
       " 'invalidated': 29896,\n",
       " 'doubts': 12225,\n",
       " 'spin': 3401,\n",
       " 'propellants': 31895,\n",
       " 'wildcat': 31909,\n",
       " 'participatory': 24407,\n",
       " 'yerushalayim': 47940,\n",
       " 'professionally': 17414,\n",
       " 'employ': 6209,\n",
       " 'decapitated': 47414,\n",
       " 'prostrate': 28281,\n",
       " 'elaborate': 4775,\n",
       " 'shirow': 31655,\n",
       " 'moraines': 35974,\n",
       " 'majin': 36081,\n",
       " 'rebus': 49564,\n",
       " 'kohl': 26452,\n",
       " 'conditioned': 14993,\n",
       " 'elmira': 42974,\n",
       " 'eighteen': 7732,\n",
       " 'zoroaster': 31455,\n",
       " 'musicianship': 35801,\n",
       " 'oxymoron': 41460,\n",
       " 'hone': 39772,\n",
       " 'hong': 1644,\n",
       " 'memoriam': 35509,\n",
       " 'mummified': 27043,\n",
       " 'democracies': 10495,\n",
       " 'conformed': 35430,\n",
       " 'yggdrasil': 41461,\n",
       " 'split': 1901,\n",
       " 'codename': 21077,\n",
       " 'dunkirk': 29756,\n",
       " 'kinetochores': 37090,\n",
       " 'outstripped': 41581,\n",
       " 'boiled': 12720,\n",
       " 'kilocalories': 46924,\n",
       " 'myocardial': 29746,\n",
       " 'inadvertently': 16354,\n",
       " 'kushner': 36743,\n",
       " 'frenchmen': 35531,\n",
       " 'vivien': 49407,\n",
       " 'qualifications': 13065,\n",
       " 'workforce': 15603,\n",
       " 'emphases': 42437,\n",
       " 'marched': 8632,\n",
       " 'boiler': 32600,\n",
       " 'issas': 41462,\n",
       " 'supper': 11207,\n",
       " 'abendana': 48681,\n",
       " 'wct': 44017,\n",
       " 'wcw': 33516,\n",
       " 'peremptory': 42436,\n",
       " 'mentors': 38939,\n",
       " 'academic': 1950,\n",
       " 'stillness': 44616,\n",
       " 'academia': 13406,\n",
       " 'academie': 45855,\n",
       " 'corporate': 2774,\n",
       " 'plaque': 10771,\n",
       " 'outlived': 33559,\n",
       " 'arcologies': 44108,\n",
       " 'appropriately': 13876,\n",
       " 'epochs': 25074,\n",
       " 'spassky': 34072,\n",
       " 'linehan': 47941,\n",
       " 'edessa': 23862,\n",
       " 'psychometrics': 43528,\n",
       " 'homogeneity': 20919,\n",
       " 'lassa': 28784,\n",
       " 'belloc': 30108,\n",
       " 'fela': 41185,\n",
       " 'portrayed': 3777,\n",
       " 'lasso': 38182,\n",
       " 'hai': 23113,\n",
       " 'ih': 32442,\n",
       " 'hak': 33316,\n",
       " 'hal': 7280,\n",
       " 'ham': 7908,\n",
       " 'han': 6937,\n",
       " 'hao': 45856,\n",
       " 'hab': 30545,\n",
       " 'espouses': 40181,\n",
       " 'had': 49,\n",
       " 'advancement': 9127,\n",
       " 'hag': 39774,\n",
       " 'keynesians': 36082,\n",
       " 'fortran': 7830,\n",
       " 'mcnamara': 24625,\n",
       " 'beloved': 7785,\n",
       " 'haq': 36728,\n",
       " 'har': 29747,\n",
       " 'has': 41,\n",
       " 'hat': 4680,\n",
       " 'hau': 48684,\n",
       " 'haw': 23968,\n",
       " 'municipal': 4576,\n",
       " 'osman': 34645,\n",
       " 'elders': 10430,\n",
       " 'survival': 4418,\n",
       " 'palahniuk': 47662,\n",
       " 'unequivocally': 31896,\n",
       " 'objective': 4057,\n",
       " 'otherworldly': 41000,\n",
       " 'indicative': 9659,\n",
       " 'clustered': 24517,\n",
       " 'shadow': 4538,\n",
       " 'hamas': 6441,\n",
       " 'istat': 39350,\n",
       " 'masonic': 11793,\n",
       " 'pennies': 36632,\n",
       " 'bernd': 36168,\n",
       " 'longbowmen': 39351,\n",
       " 'alice': 4439,\n",
       " 'niels': 16587,\n",
       " 'festivities': 21213,\n",
       " 'businessweek': 37022,\n",
       " 'misdemeanors': 30968,\n",
       " 'warping': 37091,\n",
       " 'attorney': 4889,\n",
       " 'crowd': 4942,\n",
       " 'dianetic': 22998,\n",
       " 'czech': 2597,\n",
       " 'mosques': 9578,\n",
       " 'rer': 13626,\n",
       " 'crown': 1733,\n",
       " 'topping': 22060,\n",
       " 'proportionally': 30772,\n",
       " 'deflection': 17731,\n",
       " 'captive': 11192,\n",
       " 'choctaw': 14995,\n",
       " 'billboard': 8337,\n",
       " 'fiduciary': 40568,\n",
       " 'bottom': 2189,\n",
       " 'applescript': 44617,\n",
       " 'inhuman': 26453,\n",
       " 'plucked': 21380,\n",
       " 'ichij': 36083,\n",
       " 'locksmithing': 38971,\n",
       " 'syphilis': 18627,\n",
       " 'monogamy': 17366,\n",
       " 'agadir': 43554,\n",
       " 'howell': 25497,\n",
       " 'barcode': 30333,\n",
       " 'eduard': 15240,\n",
       " 'binder': 28446,\n",
       " 'brigades': 9579,\n",
       " 'starring': 2999,\n",
       " 'anagram': 16960,\n",
       " 'accelerations': 30476,\n",
       " 'stoker': 12275,\n",
       " 'stokes': 16038,\n",
       " 'restlessness': 33803,\n",
       " 'benches': 25369,\n",
       " 'ades': 48229,\n",
       " 'anomalous': 15201,\n",
       " 'bicentennial': 29191,\n",
       " 'oneness': 20370,\n",
       " 'adel': 41973,\n",
       " 'aden': 26063,\n",
       " 'ribozymes': 47142,\n",
       " 'obscenity': 24088,\n",
       " 'lemmings': 29748,\n",
       " 'flambards': 47943,\n",
       " 'maxwell': 4681,\n",
       " 'marshall': 3143,\n",
       " 'honeymoon': 20028,\n",
       " 'nongovernmental': 42438,\n",
       " 'mba': 39352,\n",
       " 'bulgar': 42975,\n",
       " 'administer': 13026,\n",
       " 'beings': 2853,\n",
       " 'lieben': 38209,\n",
       " 'marshals': 20850,\n",
       " 'undertakings': 32671,\n",
       " 'telomeres': 42339,\n",
       " 'hallucinogenic': 19724,\n",
       " 'shoots': 11463,\n",
       " 'clarinetists': 48068,\n",
       " 'stomping': 44858,\n",
       " 'despised': 18968,\n",
       " 'fabric': 8679,\n",
       " 'tamm': 49410,\n",
       " 'altitude': 5270,\n",
       " 'diocese': 8829,\n",
       " 'raped': 20566,\n",
       " 'scheele': 38756,\n",
       " 'grasping': 30109,\n",
       " 'greatness': 15463,\n",
       " 'rapes': 44618,\n",
       " 'alexandrian': 14778,\n",
       " 'subkey': 47363,\n",
       " 'tama': 40182,\n",
       " 'distortions': 24401,\n",
       " 'denoting': 15125,\n",
       " 'thesaurus': 26185,\n",
       " 'verde': 8226,\n",
       " 'safeguard': 19829,\n",
       " 'verdi': 11955,\n",
       " 'duel': 12982,\n",
       " 'toolkits': 45857,\n",
       " 'azo': 42976,\n",
       " 'arrays': 7492,\n",
       " 'musik': 22059,\n",
       " 'cervix': 29846,\n",
       " 'facchetti': 42702,\n",
       " 'complications': 9090,\n",
       " 'pesos': 28447,\n",
       " 'smashed': 28197,\n",
       " 'duet': 15134,\n",
       " 'azt': 36391,\n",
       " 'dues': 34074,\n",
       " 'passenger': 3230,\n",
       " 'disgrace': 29749,\n",
       " 'moderne': 33804,\n",
       " 'barrymore': 17934,\n",
       " 'publica': 34844,\n",
       " 'minas': 21722,\n",
       " 'fanzine': 16551,\n",
       " 'railhead': 29052,\n",
       " 'voroshilov': 29933,\n",
       " 'signification': 38580,\n",
       " 'paperwork': 36544,\n",
       " 'pubmed': 27166,\n",
       " 'scuttled': 26558,\n",
       " 'triangles': 13529,\n",
       " 'eventual': 5943,\n",
       " 'dowling': 42977,\n",
       " 'cambodia': 5273,\n",
       " 'pasadena': 18216,\n",
       " 'role': 423,\n",
       " 'chemnitz': 40431,\n",
       " 'taney': 40183,\n",
       " 'rolf': 32601,\n",
       " 'intrusions': 37739,\n",
       " 'transgender': 15531,\n",
       " 'roll': 2598,\n",
       " 'diagonals': 43530,\n",
       " 'anisotropies': 34258,\n",
       " 'cabrera': 37989,\n",
       " 'edom': 20851,\n",
       " 'intend': 14732,\n",
       " 'palms': 25101,\n",
       " 'ointment': 39353,\n",
       " 'outage': 40184,\n",
       " 'transported': 7862,\n",
       " 'palme': 31897,\n",
       " 'conservatively': 47143,\n",
       " 'acquaintance': 14941,\n",
       " 'yellowstone': 21156,\n",
       " 'smelling': 31420,\n",
       " 'variable': 2555,\n",
       " 'transporter': 26612,\n",
       " 'aragorn': 41464,\n",
       " 'hawker': 19233,\n",
       " 'steffens': 45429,\n",
       " 'explosions': 15126,\n",
       " 'loren': 28977,\n",
       " 'dreamers': 25771,\n",
       " 'shootout': 22338,\n",
       " 'bnls': 37797,\n",
       " 'ordination': 11518,\n",
       " 'hexokinase': 47945,\n",
       " 'overturned': 15464,\n",
       " 'gown': 37092,\n",
       " 'osz': 28793,\n",
       " 'cincinnati': 4520,\n",
       " 'chaim': 23333,\n",
       " 'chain': 1952,\n",
       " 'whoever': 13044,\n",
       " 'oss': 24410,\n",
       " 'osr': 45248,\n",
       " 'osu': 45249,\n",
       " 'ost': 38429,\n",
       " 'idealistic': 24374,\n",
       " 'mongoloids': 48685,\n",
       " 'osi': 22245,\n",
       " 'osh': 41002,\n",
       " 'aleman': 47946,\n",
       " 'childe': 25921,\n",
       " 'bandits': 21976,\n",
       " 'brainchild': 37560,\n",
       " 'chair': 5404,\n",
       " 'macht': 36086,\n",
       " 'ballet': 8200,\n",
       " 'amplification': 12846,\n",
       " 'freelance': 19100,\n",
       " 'megawatts': 37437,\n",
       " 'crates': 41974,\n",
       " 'crater': 7083,\n",
       " 'macha': 34955,\n",
       " 'adventist': 26448,\n",
       " 'oversight': 14855,\n",
       " 'tenacious': 38185,\n",
       " 'nouvelle': 31148,\n",
       " 'downloading': 24981,\n",
       " 'jerk': 20029,\n",
       " 'scharnhorst': 30529,\n",
       " 'iggy': 34188,\n",
       " 'equilateral': 36078,\n",
       " 'olympus': 17841,\n",
       " 'choice': 1431,\n",
       " 'lark': 43935,\n",
       " 'embark': 28282,\n",
       " 'gloomy': 34956,\n",
       " 'rationality': 11272,\n",
       " 'knapsack': 34647,\n",
       " 'stays': 13008,\n",
       " 'bestiary': 25498,\n",
       " 'exact': 2365,\n",
       " 'aichi': 42730,\n",
       " 'minute': 3075,\n",
       " 'catalonia': 8315,\n",
       " 'talbot': 19812,\n",
       " 'cooks': 32602,\n",
       " 'reining': 45859,\n",
       " 'minnie': 13825,\n",
       " 'skewed': 25370,\n",
       " 'islets': 13703,\n",
       " 'illustrators': 25640,\n",
       " 'akrotiri': 44620,\n",
       " 'cooke': 19688,\n",
       " 'heracleidae': 39293,\n",
       " 'palestrina': 20172,\n",
       " 'multiprocessing': 28979,\n",
       " 'batsmen': 26064,\n",
       " 'espoo': 48530,\n",
       " 'meadow': 23220,\n",
       " 'adorno': 22155,\n",
       " 'trails': 12160,\n",
       " 'copyrighted': 12654,\n",
       " 'cameraman': 39775,\n",
       " 'lengthened': 28448,\n",
       " 'heavyweight': 13956,\n",
       " 'jumblatt': 32603,\n",
       " 'chopping': 33318,\n",
       " 'shirts': 13617,\n",
       " 'unmik': 23541,\n",
       " 'agonists': 27489,\n",
       " 'strictness': 36648,\n",
       " 'montoneros': 35814,\n",
       " 'headset': 37798,\n",
       " 'vorder': 43531,\n",
       " 'antwerp': 11909,\n",
       " 'celebrated': 3090,\n",
       " 'polygonal': 38581,\n",
       " 'baggins': 38941,\n",
       " 'geography': 1391,\n",
       " 'boost': 8167,\n",
       " 'gamsakhurdia': 41004,\n",
       " 'unintentionally': 27350,\n",
       " 'centripetal': 24472,\n",
       " 'annalen': 26909,\n",
       " 'drafted': 8400,\n",
       " 'oldies': 25102,\n",
       " 'climbs': 27654,\n",
       " 'honour': 4214,\n",
       " 'pedagogic': 41005,\n",
       " 'vanderbilt': 23542,\n",
       " 'gladys': 23429,\n",
       " 'address': 1308,\n",
       " 'baudot': 29750,\n",
       " 'annales': 19380,\n",
       " 'benson': 11578,\n",
       " 'mafioso': 32604,\n",
       " 'enroll': 28295,\n",
       " 'plunges': 40714,\n",
       " 'accomplishes': 41006,\n",
       " 'cyclades': 31656,\n",
       " 'mafiosi': 38216,\n",
       " 'dusty': 24518,\n",
       " 'impacted': 17732,\n",
       " 'queue': 14317,\n",
       " 'colonna': 24982,\n",
       " 'accomplished': 4809,\n",
       " 'throughput': 15315,\n",
       " 'stencil': 28909,\n",
       " 'phane': 33314,\n",
       " 'gollancz': 27059,\n",
       " 'windowing': 36731,\n",
       " 'influx': 9462,\n",
       " 'presocratic': 29934,\n",
       " 'ellensburg': 41465,\n",
       " 'prepositional': 35337,\n",
       " 'betraying': 34648,\n",
       " 'agesilaus': 36088,\n",
       " 'myoglobin': 41007,\n",
       " 'darnell': 45861,\n",
       " 'undergone': 11283,\n",
       " 'working': 742,\n",
       " 'perished': 13796,\n",
       " 'oldham': 31440,\n",
       " 'pints': 34957,\n",
       " 'optimize': 24250,\n",
       " 'redman': 40011,\n",
       " 'vigour': 34958,\n",
       " 'opposed': 1347,\n",
       " 'overviews': 22063,\n",
       " 'alastair': 32362,\n",
       " 'fluorite': 47136,\n",
       " 'assimilation': 14962,\n",
       " 'tundra': 22246,\n",
       " 'approving': 25103,\n",
       " 'thompson': 3827,\n",
       " 'consoles': 7941,\n",
       " 'tines': 44111,\n",
       " 'cardinality': 10089,\n",
       " 'riders': 10160,\n",
       " 'freyja': 40185,\n",
       " 'rebounding': 42439,\n",
       " 'lowercase': 9745,\n",
       " 'opioids': 25243,\n",
       " 'dendrite': 49731,\n",
       " 'workprint': 47949,\n",
       " 'originally': 606,\n",
       " 'rutherford': 13142,\n",
       " 'abortion': 4311,\n",
       " 'harmonious': 25772,\n",
       " 'albright': 24753,\n",
       " 'following': 222,\n",
       " 'reordering': 48662,\n",
       " 'ammonite': 26910,\n",
       " 'admired': 9542,\n",
       " 'osage': 45862,\n",
       " 'capablanca': 41975,\n",
       " 'reification': 45542,\n",
       " 'locke': 7533,\n",
       " 'mailboxes': 45863,\n",
       " 'parachute': 13989,\n",
       " 'locks': 9981,\n",
       " 'incremental': 18562,\n",
       " 'admirer': 21381,\n",
       " 'listens': 34359,\n",
       " 'litre': 12138,\n",
       " 'bibliographical': 40606,\n",
       " 'septic': 29564,\n",
       " 'vainly': 45250,\n",
       " 'haarlem': 28717,\n",
       " 'thanking': 42440,\n",
       " 'edouard': 24519,\n",
       " 'oswaldo': 33049,\n",
       " 'maude': 23493,\n",
       " 'minicomputers': 23865,\n",
       " 'flaubert': 16722,\n",
       " 'atal': 49412,\n",
       " 'custer': 31943,\n",
       " 'paleontologists': 23640,\n",
       " 'neuch': 37438,\n",
       " 'mythos': 8514,\n",
       " 'convincingly': 24856,\n",
       " 'fueled': 11281,\n",
       " 'echidna': 37439,\n",
       " 'reassessment': 37799,\n",
       " 'sociolinguistics': 44621,\n",
       " 'atat': 26065,\n",
       " 'harmon': 25641,\n",
       " 'temperate': 6633,\n",
       " 'pulley': 26323,\n",
       " 'surfing': 16457,\n",
       " 'conscious': 5248,\n",
       " 'burckhardt': 41976,\n",
       " 'quadrant': 23969,\n",
       " 'inhabiting': 16033,\n",
       " 'subdivisions': 9202,\n",
       " 'mango': 34898,\n",
       " 'forebears': 34361,\n",
       " 'swollen': 27490,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3084, 'originated', '->', 5239, 'anarchism')\n",
      "(3084, 'originated', '->', 12, 'as')\n",
      "(12, 'as', '->', 3084, 'originated')\n",
      "(12, 'as', '->', 6, 'a')\n",
      "(6, 'a', '->', 12, 'as')\n",
      "(6, 'a', '->', 195, 'term')\n",
      "(195, 'term', '->', 2, 'of')\n",
      "(195, 'term', '->', 6, 'a')\n"
     ]
    }
   ],
   "source": [
    "[reverse_dictionary[i] for i in batch], [reverse_dictionary[i] for i in labels[:, 0]]\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 271.43389892578125)\n",
      "Nearest to first: greenery, unsatisfied, shit, priscus, objectivism, interred, patricia, outer,\n",
      "Nearest to UNK: powerplant, sow, intergovernmentalism, counters, pentium, educated, tahiti, apical,\n",
      "Nearest to has: benoit, spacecraft, paradise, compressors, circ, decry, mia, etext,\n",
      "Nearest to used: modify, refreshing, decommissioned, sectors, collectivism, tidy, mattress, signifies,\n",
      "Nearest to may: walras, dartmoor, santiago, defied, service, portfolios, noting, autos,\n",
      "Nearest to so: funneled, sari, misgivings, hetzel, performances, redaction, estuary, hawaii,\n",
      "Nearest to while: watterson, tilden, bearing, alam, patenting, electron, electronegative, ige,\n",
      "Nearest to i: domingue, ghetto, ka, bcp, partita, mere, unlimited, outre,\n",
      "Nearest to were: wisconsin, displayed, stagnant, ostia, succinctly, andropov, zhdanov, curiosity,\n",
      "Nearest to by: spiegel, aqdas, netherland, metastasis, rediscovery, distinguishing, modulator, lowered,\n",
      "Nearest to he: hype, bev, kassel, crannog, backslash, method, hog, thunderbirds,\n",
      "Nearest to if: poked, vicarage, report, gott, rewards, bentley, kournikova, pop,\n",
      "Nearest to history: prospective, steelers, meriwether, predominated, adjectives, evers, girolamo, brooklyn,\n",
      "Nearest to its: formulating, trintignant, portable, greenish, electronegativity, peroxides, tupelo, esc,\n",
      "Nearest to a: irs, wit, lansdowne, cardiff, critically, cult, newbies, expressways,\n",
      "Nearest to will: reconnection, disable, woodcut, ailing, casinos, sao, pirates, recorder,\n",
      "('Average loss at step ', 2000, ': ', 113.16442505550384)\n",
      "('Average loss at step ', 4000, ': ', 52.546211807250977)\n",
      "('Average loss at step ', 6000, ': ', 33.443033806920049)\n",
      "('Average loss at step ', 8000, ': ', 23.762722144126894)\n",
      "('Average loss at step ', 10000, ': ', 18.265776746153833)\n",
      "Nearest to first: objectivism, achill, priscus, akira, both, in, interred, settles,\n",
      "Nearest to UNK: and, one, the, vs, reginae, guilty, mathbf, basins,\n",
      "Nearest to has: is, paradise, spacecraft, strength, authors, mia, altenberg, reginae,\n",
      "Nearest to used: metaphysical, collectivism, consists, modify, sectors, jpg, selden, amo,\n",
      "Nearest to may: nine, fins, leader, tongue, ghosts, service, kournikova, aol,\n",
      "Nearest to so: hawaii, performances, fao, rudolph, vs, england, followed, legs,\n",
      "Nearest to while: electron, bearing, amo, aquarius, lot, beginning, allies, mans,\n",
      "Nearest to i: reginae, ghetto, chloride, abdali, trick, mere, ka, economics,\n",
      "Nearest to were: are, plaster, displayed, earlier, wisconsin, experience, encounters, retreated,\n",
      "Nearest to by: in, as, and, from, users, dogmatic, two, phi,\n",
      "Nearest to he: it, crannog, killed, been, cracking, flanders, gollancz, correctly,\n",
      "Nearest to if: report, pop, abwehr, excavation, voice, ages, vicarage, kournikova,\n",
      "Nearest to history: prospective, steelers, crowd, holocaust, good, vs, upon, brooklyn,\n",
      "Nearest to its: the, one, becoming, selznick, offensive, resigned, sr, portable,\n",
      "Nearest to a: the, and, UNK, mathbf, cardiff, fins, gb, another,\n",
      "Nearest to will: victoriae, barbuda, and, printed, asteroid, reflection, transcontinental, fins,\n",
      "('Average loss at step ', 12000, ': ', 14.056990735054017)\n",
      "('Average loss at step ', 14000, ': ', 11.810415421843528)\n",
      "('Average loss at step ', 16000, ': ', 9.8365392937660214)\n",
      "('Average loss at step ', 18000, ': ', 8.6671083641052249)\n",
      "('Average loss at step ', 20000, ': ', 7.7571109436750412)\n",
      "Nearest to first: agouti, objectivism, in, achill, interred, both, zero, akira,\n",
      "Nearest to UNK: agouti, dasyprocta, and, vs, two, four, hbox, three,\n",
      "Nearest to has: is, had, was, paradise, circ, compiler, have, authors,\n",
      "Nearest to used: metaphysical, dasyprocta, collectivism, browed, tenth, ashmore, consists, amo,\n",
      "Nearest to may: nine, fins, defied, seven, six, limb, tongue, would,\n",
      "Nearest to so: hawaii, dasyprocta, he, palatal, fao, performances, hg, followed,\n",
      "Nearest to while: bearing, electron, amo, lf, sitting, from, and, aquarius,\n",
      "Nearest to i: UNK, reginae, abdali, ghetto, chloride, trick, mere, picks,\n",
      "Nearest to were: are, was, is, by, plaster, displayed, yin, had,\n",
      "Nearest to by: as, in, from, for, and, was, with, two,\n",
      "Nearest to he: it, backslash, they, and, been, eight, nine, crannog,\n",
      "Nearest to if: pop, report, excavation, abwehr, vicarage, kbit, already, ages,\n",
      "Nearest to history: prospective, agouti, annales, crowd, brooklyn, dasyprocta, girardeau, edicts,\n",
      "Nearest to its: the, his, their, a, agouti, one, offensive, abadan,\n",
      "Nearest to a: the, agouti, another, dasyprocta, this, or, one, its,\n",
      "Nearest to will: and, recorder, agouti, printed, produces, sunda, fins, dasyprocta,\n",
      "('Average loss at step ', 22000, ': ', 7.2026611722707745)\n",
      "('Average loss at step ', 24000, ': ', 6.9355187491178514)\n",
      "('Average loss at step ', 26000, ': ', 6.6781790490150454)\n",
      "('Average loss at step ', 28000, ': ', 6.249697914242744)\n",
      "('Average loss at step ', 30000, ': ', 6.160901928067207)\n",
      "Nearest to first: agouti, objectivism, in, interred, achill, both, patricia, with,\n",
      "Nearest to UNK: agouti, dasyprocta, tunings, vs, four, abakan, backslash, reginae,\n",
      "Nearest to has: had, is, was, have, paradise, compiler, circ, altenberg,\n",
      "Nearest to used: metaphysical, torgau, dasyprocta, aba, browed, decommissioned, collectivism, expand,\n",
      "Nearest to may: would, can, nine, categorised, defied, six, fins, seven,\n",
      "Nearest to so: abitibi, but, hawaii, dasyprocta, hg, palatal, fao, he,\n",
      "Nearest to while: and, aba, bearing, were, from, electron, amo, ige,\n",
      "Nearest to i: UNK, reginae, ghetto, abdali, chloride, trick, mere, four,\n",
      "Nearest to were: are, was, is, by, had, plaster, while, be,\n",
      "Nearest to by: in, as, and, from, with, was, for, abet,\n",
      "Nearest to he: it, they, backslash, she, who, and, there, then,\n",
      "Nearest to if: when, pop, report, doubtful, excavation, vicarage, bentley, abwehr,\n",
      "Nearest to history: prospective, agouti, aba, chaldean, brooklyn, annales, crowd, steelers,\n",
      "Nearest to its: the, their, his, a, agouti, pairing, offensive, bpp,\n",
      "Nearest to a: the, agouti, dasyprocta, another, this, or, vojt, abitibi,\n",
      "Nearest to will: would, can, and, might, or, primigenius, anthroposophy, must,\n",
      "('Average loss at step ', 32000, ': ', 5.8942106403112415)\n",
      "('Average loss at step ', 34000, ': ', 5.8399711575508118)\n",
      "('Average loss at step ', 36000, ': ', 5.6933053689002993)\n",
      "('Average loss at step ', 38000, ': ', 5.2934979794025425)\n",
      "('Average loss at step ', 40000, ': ', 5.4946520501375202)\n",
      "Nearest to first: agouti, objectivism, achill, interred, both, patricia, unsatisfied, second,\n",
      "Nearest to UNK: dasyprocta, agouti, and, backslash, tunings, three, one, reginae,\n",
      "Nearest to has: had, is, was, have, compiler, circ, paradise, altenberg,\n",
      "Nearest to used: metaphysical, dasyprocta, torgau, aba, amo, reuptake, browed, galois,\n",
      "Nearest to may: can, would, categorised, will, nine, defied, to, should,\n",
      "Nearest to so: abitibi, but, transferring, it, threatening, he, dasyprocta, palatal,\n",
      "Nearest to while: and, aba, were, bearing, from, ige, amo, electron,\n",
      "Nearest to i: reginae, UNK, t, ghetto, abdali, trick, chloride, picks,\n",
      "Nearest to were: are, was, is, by, had, while, have, be,\n",
      "Nearest to by: was, from, as, with, were, in, hellfire, and,\n",
      "Nearest to he: it, she, they, who, backslash, there, then, later,\n",
      "Nearest to if: when, pop, abwehr, vicarage, bentley, excavation, doubtful, ing,\n",
      "Nearest to history: prospective, agouti, aba, chaldean, brooklyn, steelers, annales, peptide,\n",
      "Nearest to its: their, the, his, pairing, agouti, a, butler, some,\n",
      "Nearest to a: the, agouti, abitibi, another, dasyprocta, this, vojt, eight,\n",
      "Nearest to will: would, can, may, must, might, and, to, sed,\n",
      "('Average loss at step ', 42000, ': ', 5.2884035185575486)\n",
      "('Average loss at step ', 44000, ': ', 5.3145419737100603)\n",
      "('Average loss at step ', 46000, ': ', 5.2635524272918701)\n",
      "('Average loss at step ', 48000, ': ', 5.0368763834238051)\n",
      "('Average loss at step ', 50000, ': ', 5.1556160471439361)\n",
      "Nearest to first: objectivism, second, agouti, interred, patricia, next, achill, greenery,\n",
      "Nearest to UNK: agouti, dasyprocta, four, reginae, three, victoriae, backslash, reuptake,\n",
      "Nearest to has: had, is, was, have, biconditional, compiler, altenberg, already,\n",
      "Nearest to used: metaphysical, prat, dasyprocta, torgau, aba, reuptake, amo, galois,\n",
      "Nearest to may: can, would, will, categorised, should, nine, to, seven,\n",
      "Nearest to so: abitibi, but, threatening, transferring, residues, palatal, dasyprocta, hg,\n",
      "Nearest to while: and, aba, from, thibetanus, were, bearing, though, but,\n",
      "Nearest to i: t, reginae, ghetto, UNK, abdali, he, trick, immensely,\n",
      "Nearest to were: are, was, is, had, have, be, being, by,\n",
      "Nearest to by: was, with, from, as, thibetanus, for, in, be,\n",
      "Nearest to he: it, she, they, who, there, backslash, then, this,\n",
      "Nearest to if: when, pop, but, abwehr, after, vicarage, for, doubtful,\n",
      "Nearest to history: prospective, aba, agouti, steelers, chaldean, brooklyn, peptide, annales,\n",
      "Nearest to its: their, the, his, pairing, agouti, roshan, a, escalation,\n",
      "Nearest to a: the, another, agouti, this, vojt, dasyprocta, eight, gourd,\n",
      "Nearest to will: would, can, may, must, might, could, to, and,\n",
      "('Average loss at step ', 52000, ': ', 5.1628256613016132)\n",
      "('Average loss at step ', 54000, ': ', 5.0801254163980483)\n",
      "('Average loss at step ', 56000, ': ', 5.0409505732059481)\n",
      "('Average loss at step ', 58000, ': ', 5.1087362531423572)\n",
      "('Average loss at step ', 60000, ': ', 4.9483141461610796)\n",
      "Nearest to first: second, objectivism, agouti, interred, next, patricia, callithrix, greenery,\n",
      "Nearest to UNK: callithrix, agouti, dasyprocta, tamarin, cebus, ssbn, microsite, marmoset,\n",
      "Nearest to has: had, have, is, was, biconditional, compiler, altenberg, enables,\n",
      "Nearest to used: metaphysical, prat, dasyprocta, tamarin, aba, reuptake, galois, amo,\n",
      "Nearest to may: can, would, will, categorised, should, could, might, nine,\n",
      "Nearest to so: abitibi, but, threatening, transferring, residues, dasyprocta, palatal, excavation,\n",
      "Nearest to while: and, or, aba, thibetanus, though, but, were, however,\n",
      "Nearest to i: t, reginae, ghetto, UNK, abdali, chasing, trick, ssbn,\n",
      "Nearest to were: are, was, have, had, is, being, be, while,\n",
      "Nearest to by: was, with, as, ssbn, be, thibetanus, from, abet,\n",
      "Nearest to he: it, she, they, who, there, backslash, then, scranton,\n",
      "Nearest to if: when, but, callithrix, pop, doubtful, after, vicarage, for,\n",
      "Nearest to history: aba, prospective, agouti, tamarin, steelers, dasyprocta, peptide, solicitation,\n",
      "Nearest to its: their, his, the, pairing, some, agouti, cebus, butler,\n",
      "Nearest to a: the, another, callithrix, marmoset, vojt, agouti, cebus, this,\n",
      "Nearest to will: would, can, may, must, might, could, to, microcebus,\n",
      "('Average loss at step ', 62000, ': ', 4.7917307525873181)\n",
      "('Average loss at step ', 64000, ': ', 4.8071960885524749)\n",
      "('Average loss at step ', 66000, ': ', 4.9775575051307674)\n",
      "('Average loss at step ', 68000, ': ', 4.9157073162794109)\n",
      "('Average loss at step ', 70000, ': ', 4.7652808930873869)\n",
      "Nearest to first: second, objectivism, next, agouti, interred, callithrix, greenery, patricia,\n",
      "Nearest to UNK: callithrix, agouti, dasyprocta, cebus, tamarin, ssbn, tunings, microsite,\n",
      "Nearest to has: had, have, is, was, biconditional, naaman, altenberg, already,\n",
      "Nearest to used: metaphysical, dasyprocta, prat, tamarin, reuptake, amo, aba, known,\n",
      "Nearest to may: can, would, will, could, should, might, categorised, must,\n",
      "Nearest to so: abitibi, thaler, transferring, threatening, residues, but, dasyprocta, palatal,\n",
      "Nearest to while: and, or, however, thibetanus, aba, but, though, were,\n",
      "Nearest to i: t, UNK, reginae, ghetto, you, we, they, abdali,\n",
      "Nearest to were: are, was, have, had, be, being, is, while,\n",
      "Nearest to by: was, specimens, with, ssbn, thibetanus, as, be, from,\n",
      "Nearest to he: it, she, they, who, there, backslash, then, scranton,\n",
      "Nearest to if: when, for, callithrix, abwehr, although, doubtful, after, but,\n",
      "Nearest to history: prospective, aba, agouti, steelers, tamarin, peptide, chaldean, dasyprocta,\n",
      "Nearest to its: their, his, the, pairing, some, cebus, escalation, agouti,\n",
      "Nearest to a: the, another, marmoset, agouti, callithrix, cebus, vojt, abitibi,\n",
      "Nearest to will: would, can, may, must, could, might, to, should,\n",
      "('Average loss at step ', 72000, ': ', 4.8034837681055071)\n",
      "('Average loss at step ', 74000, ': ', 4.7866865113079546)\n",
      "('Average loss at step ', 76000, ': ', 4.848878457486629)\n",
      "('Average loss at step ', 78000, ': ', 4.8069599908590313)\n",
      "('Average loss at step ', 80000, ': ', 4.8158067229986194)\n",
      "Nearest to first: second, cegep, next, objectivism, callithrix, triangles, agouti, interred,\n",
      "Nearest to UNK: agouti, callithrix, dasyprocta, upanija, cegep, tamarin, three, cebus,\n",
      "Nearest to has: had, have, is, was, biconditional, naaman, absalom, altenberg,\n",
      "Nearest to used: known, dasyprocta, metaphysical, prat, tamarin, amo, reuptake, aba,\n",
      "Nearest to may: can, would, will, could, should, might, must, categorised,\n",
      "Nearest to so: abitibi, thaler, transferring, threatening, residues, dasyprocta, palatal, excavation,\n",
      "Nearest to while: and, however, but, thibetanus, or, aba, though, upanija,\n",
      "Nearest to i: UNK, t, reginae, you, we, ghetto, umbilical, they,\n",
      "Nearest to were: are, was, have, had, be, being, is, while,\n",
      "Nearest to by: was, specimens, be, with, ssbn, thibetanus, as, from,\n",
      "Nearest to he: it, she, they, who, there, backslash, later, scranton,\n",
      "Nearest to if: when, callithrix, candide, although, abwehr, for, bracing, after,\n",
      "Nearest to history: prospective, aba, steelers, agouti, peptide, tamarin, chaldean, haman,\n",
      "Nearest to its: their, his, the, pairing, escalation, cebus, some, cegep,\n",
      "Nearest to a: the, another, marmoset, cegep, agouti, callithrix, cebus, microsite,\n",
      "Nearest to will: would, can, may, must, could, might, should, to,\n",
      "('Average loss at step ', 82000, ': ', 4.8188700177669528)\n",
      "('Average loss at step ', 84000, ': ', 4.7771393737792973)\n",
      "('Average loss at step ', 86000, ': ', 4.7510095541477204)\n",
      "('Average loss at step ', 88000, ': ', 4.6768265293836597)\n",
      "('Average loss at step ', 90000, ': ', 4.7664554728269577)\n",
      "Nearest to first: second, cegep, next, callithrix, agouti, objectivism, interred, priscus,\n",
      "Nearest to UNK: callithrix, dasyprocta, tamarin, agouti, upanija, cegep, microsite, cebus,\n",
      "Nearest to has: had, have, is, was, biconditional, already, fsm, naaman,\n",
      "Nearest to used: known, dasyprocta, metaphysical, boutros, prat, considered, tamarin, reuptake,\n",
      "Nearest to may: can, would, will, could, should, might, must, categorised,\n",
      "Nearest to so: transferring, abitibi, thaler, threatening, residues, palatal, otherwise, dasyprocta,\n",
      "Nearest to while: and, however, but, though, thibetanus, or, although, aba,\n",
      "Nearest to i: t, reginae, you, we, they, g, umbilical, ssbn,\n",
      "Nearest to were: are, was, had, have, being, be, while, is,\n",
      "Nearest to by: specimens, was, with, thibetanus, through, ssbn, be, for,\n",
      "Nearest to he: it, she, they, there, who, then, backslash, but,\n",
      "Nearest to if: when, callithrix, for, candide, although, where, agouti, bracing,\n",
      "Nearest to history: aba, prospective, steelers, tamarin, peptide, agouti, microsite, haman,\n",
      "Nearest to its: their, his, the, pairing, boutros, some, escalation, cebus,\n",
      "Nearest to a: the, another, marmoset, cegep, callithrix, agouti, cebus, dasyprocta,\n",
      "Nearest to will: would, can, may, must, could, might, should, to,\n",
      "('Average loss at step ', 92000, ': ', 4.7146851714849474)\n",
      "('Average loss at step ', 94000, ': ', 4.6265769623517992)\n",
      "('Average loss at step ', 96000, ': ', 4.7305508151054383)\n",
      "('Average loss at step ', 98000, ': ', 4.6262534256279473)\n",
      "('Average loss at step ', 100000, ': ', 4.6827038342952729)\n",
      "Nearest to first: second, cegep, next, callithrix, agouti, objectivism, interred, crb,\n",
      "Nearest to UNK: callithrix, agouti, dasyprocta, tamarin, marmoset, cegep, tunings, abitibi,\n",
      "Nearest to has: had, have, is, was, biconditional, fsm, already, altenberg,\n",
      "Nearest to used: known, dasyprocta, considered, boutros, prat, tamarin, reuptake, metaphysical,\n",
      "Nearest to may: can, would, will, could, should, must, might, categorised,\n",
      "Nearest to so: abitibi, transferring, thaler, threatening, otherwise, residues, dasyprocta, palatal,\n",
      "Nearest to while: and, but, however, though, thibetanus, although, when, aba,\n",
      "Nearest to i: t, you, we, reginae, they, g, umbilical, microinstruction,\n",
      "Nearest to were: are, was, have, had, be, being, while, is,\n",
      "Nearest to by: with, through, specimens, was, be, as, thibetanus, gnutella,\n",
      "Nearest to he: it, she, they, there, who, later, backslash, this,\n",
      "Nearest to if: when, where, although, for, callithrix, candide, before, abwehr,\n",
      "Nearest to history: aba, prospective, steelers, tamarin, peptide, agouti, microsite, chaldean,\n",
      "Nearest to its: their, his, the, pairing, boutros, escalation, her, cebus,\n",
      "Nearest to a: another, the, cegep, marmoset, agouti, cebus, callithrix, superfluid,\n",
      "Nearest to will: would, can, may, must, could, might, should, to,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print(\"Initialized\")\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step \", step, \": \", average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = \"%s %s,\" % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1657137   0.03434748  0.06846502  0.0070382   0.0350267   0.03630972\n",
      "  0.13870713 -0.08952355 -0.01397907  0.10646137  0.20203994 -0.02784512\n",
      "  0.00405212  0.07034153 -0.08157731 -0.09949311 -0.06979465  0.07609912\n",
      "  0.126651    0.1981343  -0.01641454  0.10250082  0.01758348 -0.04216308\n",
      " -0.15933873  0.07945434 -0.01796242 -0.00106697 -0.02517156 -0.12261134\n",
      "  0.11137157  0.03639388 -0.1277702   0.00541975  0.07960305  0.03566069\n",
      " -0.04528071 -0.09769962  0.06677049 -0.02655403 -0.03063397  0.04496832\n",
      " -0.06530884  0.06109088 -0.04010385  0.01271805  0.05815612 -0.16572201\n",
      "  0.00262617 -0.03960225 -0.09188902  0.14104255  0.12343952  0.08305998\n",
      " -0.04416177 -0.09624919 -0.09049979 -0.10931598  0.20486616 -0.00828655\n",
      "  0.13724695  0.03226418  0.05047261  0.06022134  0.12492619  0.15182598\n",
      " -0.06894512 -0.02406714  0.07673304  0.02638871  0.02326527 -0.07070541\n",
      " -0.09337375 -0.08966452  0.16761334 -0.12091971 -0.02916223 -0.06994145\n",
      " -0.07964484 -0.01511631  0.03522532  0.06124158  0.02416194  0.0876618\n",
      "  0.13634762  0.00068257  0.02688454  0.02144787  0.02604954  0.26445743\n",
      "  0.03154384  0.07350118 -0.03664459  0.07665622 -0.00189574 -0.11288846\n",
      "  0.16751142 -0.00503308  0.02526451  0.01647182 -0.13779892  0.05948371\n",
      "  0.01406884  0.00629795 -0.11318459 -0.11795699  0.19943622 -0.15585676\n",
      "  0.02766428 -0.05309179 -0.08386849 -0.12239943  0.03040705  0.01117246\n",
      " -0.05888936 -0.02056031 -0.08114543  0.10561201 -0.12420179 -0.01755991\n",
      " -0.00239956 -0.02879796 -0.0455617  -0.02028885 -0.00581089 -0.04240141\n",
      "  0.09448038 -0.08710189]\n",
      "-1.19209246918e-07\n",
      "0.772016452075\n",
      "0.864356577396\n",
      "0.675594030214\n"
     ]
    }
   ],
   "source": [
    "final_embeddings[dictionary['flow']]\n",
    "import scipy.spatial \n",
    "\n",
    "print(final_embeddings[dictionary['flow']])\n",
    "print(scipy.spatial.distance.cosine(final_embeddings[dictionary['flow']], final_embeddings[dictionary['flow']]))\n",
    "\n",
    "print(scipy.spatial.distance.cosine(final_embeddings[dictionary['tensor']], final_embeddings[dictionary['flow']]))\n",
    "print(scipy.spatial.distance.cosine(final_embeddings[dictionary['stream']], final_embeddings[dictionary['flow']]))\n",
    "print(scipy.spatial.distance.cosine(final_embeddings[dictionary['red']], final_embeddings[dictionary['blue']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "  print(\"Please install sklearn, matplotlib, and scipy to visualize embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
